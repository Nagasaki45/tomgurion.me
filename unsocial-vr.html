<!DOCTYPE html>
<html lang="en-US">
    <head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="author" content="Tom Gurion">
<meta name="description" content="Musician, coder, and researcher of head nods. Collecting hobbies in my spare time">
<title>Tom Gurion</title>
<!-- Favicon -->
<link rel="shortcut icon" href="https://tomgurion.me/favicon.ico">
<!-- Bootstrap Core CSS -->
<link rel="stylesheet" href="https://tomgurion.me/theme/css/bootstrap.min.css" type="text/css">
<!-- Custom Fonts -->
<link href='https://fonts.googleapis.com/css?family=Montserrat:300italic,400italic,700italic,300,400,700|Sen:700' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="https://tomgurion.me/theme/font-awesome/css/all.css" type="text/css">
<!-- Plugin CSS -->
<link rel="stylesheet" href="https://tomgurion.me/theme/css/youtube.css" type="text/css">
<!-- Custom CSS -->
<link rel="stylesheet" href="https://tomgurion.me/theme/css/creative.css" type="text/css">

<!-- Open graph protocol -->
<meta property="og:title" content="Tom Gurion" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://tomgurion.me" />
<meta property="og:image:secure_url" itemprop="image" content="https://tomgurion.me/images/me.jpg" />
<meta property="og:description" content="Musician, coder, and researcher of head nods. Collecting hobbies in my spare time" />

<!-- Twitter cards -->
<meta name="twitter:card" content="summary" />
<meta name="twitter:image" content="https://tomgurion.me/images/me.jpg" />    </head>
    <body id="page-top">
    <!-- Header -->
    <section>
        <div class="container article-container">
            <a class="btn btn-primary btn-home" href="/">Tom Gurion</a>
            <h1 class="title">Unsocial VR</h1>
            <img src="images/portfolio/unsocial-vr.png" />
            <div class="content">
                <!-- Content -->
                <p>Advance Project Placement, Media and Arts Technology doctoral programme, Queen Mary University of London.
Supervised by Prof. Patrick Healey and hosted by <a class="reference external" href="https://www.inition.co.uk/">Inition</a>.</p>
<div class="youtube youtube-16x9"><iframe src="https://www.youtube.com/embed/tqbtOL5R4fw" allowfullscreen seamless frameBorder="0"></iframe></div><div class="section" id="what">
<h2>What?</h2>
<p>In Infinite Jest, David Foster Wallace argues that <em>&quot;Good old traditional audio-only phone conversations allowed you to presume that the person on the other end was paying complete attention to you while also permitting you not to have to pay anything even close to complete attention to her.&quot;</em>
He continues and claims that we are addicted to this illusion, and that's why video conferencing always feel so awkward - we need to pretend to listen all the time.
And if we think about it, even in face to face conversation we must always adhere to these social rules, and signal our complete attention when someone is talking to us.</p>
<p>In this project I experiment with VR technologies to see if this illusion of faking active listening is transferable to other mediums, and if so, how.
In Unsocial VR participants share the same virtual environment, using the HTC Vive headset and controllers.
They can converse freely and move around, and if you want to start faking listening to the conversation and just wander around, or even talk with other participants while faking, you absolutely can!
The interface is very minimal, just hit a button on the controller to start faking active listening behaviours towards your current conversation, and release it when you want to stop faking.
You will even get an on screen notification when someone is speaking directly to you, so you can return to the conversation elegantly.</p>
<p>This project is based on multidisciplinary research.
It merges ideas from telepresence and mediated communication, that explore ways to represent non verbal cues in new communication technologies.
It depends on multiparty social interaction analysis, which take spatial cues into account in understanding who is talking with whom in a &quot;cocktail party&quot; situation.
It explores new ways to generate active listening signals (or &quot;backchannel behaviours&quot;, as they called in the scientific literature), such as head nods, in automated agents.</p>
<p>Further information can be found in my <a class="reference external" href="https://youtu.be/K39_wlQ60-Y">mid-project</a> and <a class="reference external" href="https://youtu.be/2k8MO74guTA">inter/sections</a> presentations.</p>
</div>

                <!-- Links -->
<div>
    <a class="btn btn-primary" href="https://github.com/Nagasaki45/UnsocialVR" target="_blank">
        <span class="fab fa-github"></span> source code
    </a>
</div>            </div>
        </div>
    </section>

<!-- Bootstrap Core JavaScript -->
<script src="https://tomgurion.me/theme/js/jquery.js"></script>
<script src="https://tomgurion.me/theme/js/bootstrap.min.js"></script>    </body>
</html>